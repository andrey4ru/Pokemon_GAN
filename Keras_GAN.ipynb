{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4zgQuj7USr0J"
   },
   "source": [
    "Mounting google drive for downloading datasets and models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42712,
     "status": "ok",
     "timestamp": 1552724710168,
     "user": {
      "displayName": "Андрей",
      "photoUrl": "",
      "userId": "10919957261353634416"
     },
     "user_tz": -120
    },
    "id": "Zxjnemxp9sYt",
    "outputId": "35fb3980-b62c-4ef2-83f9-c3c5e5eb9d97"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BoyisRUPtuK"
   },
   "outputs": [],
   "source": [
    "!rm -r pokemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHs8TrkoYVLj"
   },
   "outputs": [],
   "source": [
    "!rm -r drive/My\\ Drive/Colab\\ Notebooks/GAN/generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrp5BYcbTGyH"
   },
   "source": [
    "Unzip dataset from google drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PheP60fp4Pxv"
   },
   "outputs": [],
   "source": [
    "!unzip drive/My\\ Drive/Colab\\ Notebooks/GAN/pokemon_expanded.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hS4iA8RfTcuG"
   },
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1738,
     "status": "ok",
     "timestamp": 1552724786460,
     "user": {
      "displayName": "Андрей",
      "photoUrl": "",
      "userId": "10919957261353634416"
     },
     "user_tz": -120
    },
    "id": "E8tRy_zC3qu9",
    "outputId": "94e3fb74-3796-4a5a-ef56-ae553e38e2a4"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, Conv3D, UpSampling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from scipy.misc import imsave as ims\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQqBoucgTUEm"
   },
   "source": [
    "Downloading images from folder, '.jpg' format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnXwVqLaAjUJ"
   },
   "outputs": [],
   "source": [
    "def load_images_to_data(image_directory, im_dim):\n",
    "    features_data = np.zeros((1, im_dim, im_dim, 3))\n",
    "    list_of_files = os.listdir(image_directory)\n",
    "    for file in list_of_files:\n",
    "        image_file_name = os.path.join(image_directory, file)\n",
    "        if \".jpg\" in image_file_name:\n",
    "            img = Image.open(image_file_name).resize((im_dim, im_dim)).convert(\"RGB\")\n",
    "\n",
    "            im2arr = np.asarray(img).reshape(1, im_dim, im_dim, 3)\n",
    "            features_data = np.append(features_data, im2arr, axis=0)\n",
    "        \n",
    "    return features_data[1:,: ,: ,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oPqSlqkkTvWf"
   },
   "source": [
    "Function return generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTN52ZAf4g7f"
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "\n",
    "    noise_shape = (100,)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128 * 16 * 16, activation=\"relu\", input_shape=noise_shape))\n",
    "    model.add(Reshape((16, 16, 128)))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Conv2D(3, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    noise = Input(shape=noise_shape)\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bcvGVXg4T112"
   },
   "source": [
    "Function return discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N2hz5vHG4hDf"
   },
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c14RlpacT5e-"
   },
   "source": [
    "Saving images, combine dim*dim images per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jW0fO3VD7eIp"
   },
   "outputs": [],
   "source": [
    "def save_imgs(gen, epoch, save_folder, dim):\n",
    "    r, c = dim, dim\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = gen.predict(noise)\n",
    "\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "    ims(save_folder + 'images/pokemon_' +  epoch + '.png', merge(gen_imgs,[dim, dim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KO5iEQFh8bao"
   },
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcmvP30qUFb9"
   },
   "source": [
    "Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pSSdXPsN45kv"
   },
   "outputs": [],
   "source": [
    "def train(gen, dis, comb, epochs, batch_size, save_folder, X_train):\n",
    "  \n",
    "    iterations = 0\n",
    "    save_interval = 200\n",
    "    \n",
    "    # Scale to -1, 1\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "    half_batch = int(batch_size / 2)\n",
    "\n",
    "    for epoch in range (epochs + 1):\n",
    "      for index in range(int(X_train.shape[0]/half_batch)):\n",
    "          # ---------------------\n",
    "          #  Train Discriminator\n",
    "          # ---------------------\n",
    "\n",
    "#         idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "#         imgs = X_train[idx]\n",
    "        # Get batch\n",
    "        imgs = X_train[index * half_batch : (index + 1) * (half_batch)]\n",
    "\n",
    "        # Generate new images\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "        gen_imgs = gen.predict(noise)\n",
    "\n",
    "        # Discriminator trainning\n",
    "        dis_loss_real = dis.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "        dis_loss_fake = dis.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "        dis_loss = 0.5 * np.add(dis_loss_real, dis_loss_fake)\n",
    "#           dis_los = self.discriminator.train_on_batch(train_batch, target)\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "        # Generator training\n",
    "        gen_loss = comb.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "        # Logs\n",
    "#               if epoch % save_interval == 0 & index == 0:\n",
    "        if iterations % save_interval == 0:\n",
    "#                 print(\"iteration %f, epoch %f\" % (iterations, epoch))\n",
    "          print (\"epoch: %d iter: %d D loss: %f, acc.: %.2f%% | G loss: %f\" % \n",
    "                 (epoch, iterations, dis_loss[0], 100*dis_loss[1], gen_loss))\n",
    "\n",
    "#         if epoch == 0:\n",
    "        if iterations == 0:\n",
    "            model_json = gen.to_json()\n",
    "            with open(save_folder + \"weights/generator.json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "\n",
    "            model_dis_json = dis.to_json()\n",
    "            with open(save_folder + \"weights/discriminator.json\", \"w\") as json_file:\n",
    "                json_file.write(model_dis_json)\n",
    "        \n",
    "        # Save models & generated images\n",
    "        if iterations % save_interval == 0:\n",
    "#         if epoch % save_interval == 0:\n",
    "            save_imgs(gen, str(epoch), save_folder, 1)\n",
    "            gen_name = save_folder + \"weights/gen_\" + str(epoch) + \".h5\"\n",
    "            gen.save_weights(gen_name)\n",
    "\n",
    "            dis_name = save_folder + \"weights/dis_\" + str(epoch) + \".h5\"\n",
    "            dis.save_weights(dis_name)\n",
    "\n",
    "        iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jEXq4IabVGtD"
   },
   "source": [
    "Models initialization, return discriminator, generator, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_wBPPGk57NF"
   },
   "outputs": [],
   "source": [
    " def init_model():\n",
    "    img_rows = 64\n",
    "    img_cols = 64\n",
    "    channels = 3\n",
    "\n",
    "    optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "    # Build and compile the discriminator\n",
    "    discriminator = build_discriminator((img_rows, img_cols, channels))\n",
    "    discriminator.compile(loss='binary_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy'])\n",
    "#           metrics=['mae'])\n",
    "\n",
    "    # Build and compile the generator\n",
    "    generator = build_generator()\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    # The generator takes noise as input and generated imgs\n",
    "    z = Input(shape=(100,))\n",
    "    img = generator(z)\n",
    "\n",
    "    # For the combined model we will only train the generator\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    # The valid takes generated images as input and determines validity\n",
    "    valid = discriminator(img)\n",
    "\n",
    "    # The combined model  (stacked generator and discriminator) takes\n",
    "    # noise as input => generates images => determines validity\n",
    "    combined = Model(z, valid)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return discriminator, generator, combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xAdmqkF6VSaG"
   },
   "source": [
    "Load model from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lq1F1p59SC8n"
   },
   "outputs": [],
   "source": [
    "def load_model(model_folder, model_name, model_type):\n",
    "  # load json and create model\n",
    "    json_file = open(model_folder + 'weights/' + model_type + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(model_folder+ 'weights/' + model_name)#\"weights/model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oH8ytZE_VXJT"
   },
   "source": [
    "Load pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9zPShO7R4OR"
   },
   "outputs": [],
   "source": [
    " def load_pretrained_models(models_folder, dis, gen):\n",
    "    img_rows = 64\n",
    "    img_cols = 64\n",
    "    channels = 3\n",
    "\n",
    "    optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "#     models_folder = 'drive/My Drive/Colab Notebooks/GAN/'\n",
    "\n",
    "\n",
    "    # Build the discriminator\n",
    "    discriminator = load_model(models_folder, dis, 'discriminator')\n",
    "    discriminator.compile(loss='binary_crossentropy',\n",
    "      optimizer=optimizer,\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "    # Build a the generator\n",
    "    generator = load_model(models_folder, gen, 'generator')\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    # The generator takes noise as input and generated imgs\n",
    "    z = Input(shape=(100,))\n",
    "    img = generator(z)\n",
    "\n",
    "    # For the combined model we will only train the generator\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    \n",
    "    # The valid takes generated images as input and determines validity\n",
    "    valid = discriminator(img)\n",
    "\n",
    "    # The combined model  (stacked generator and discriminator) takes\n",
    "    # noise as input => generates images => determines validity\n",
    "    combined = Model(z, valid)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return discriminator, generator, combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jr7xrfZDVjQk"
   },
   "source": [
    "Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C__jFlKHnRJT"
   },
   "outputs": [],
   "source": [
    "# save_folder = 'drive/My Drive/Colab Notebooks/GAN/'\n",
    "save_folder = ''\n",
    "\n",
    "if not os.path.exists(save_folder + '/images/'):\n",
    "    os.makedirs(save_folder + '/images/')\n",
    "    \n",
    "if not os.path.exists(save_folder + '/generated/'):\n",
    "    os.makedirs(save_folder + '/generated/')\n",
    "\n",
    "if not os.path.exists(save_folder + '/weights/'):\n",
    "    os.makedirs(save_folder + '/weights/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4PdNGl2XVuFS"
   },
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F8zwodaVY45C"
   },
   "outputs": [],
   "source": [
    "data_train = load_images_to_data('pokemon', 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G49XApzEVywi"
   },
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMZVNici6OCt"
   },
   "outputs": [],
   "source": [
    "batchsize = 50\n",
    "epochs = 200\n",
    "\n",
    "save_folder = ''\n",
    "# dis = 'dis_100.h5'\n",
    "# gen = 'gen_100.h5'\n",
    "# discriminator, generator, combined = load_pretrained_models(save_folder + 'AUG_100epochs_64/', dis, gen)\n",
    "discriminator, generator, combined = init_model()\n",
    "train(generator, discriminator, combined, epochs, batchsize, save_folder, data_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "59VL_L79V2-V"
   },
   "source": [
    "Function generating new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "glyjHbKBMlnI"
   },
   "outputs": [],
   "source": [
    "def generate_image(save_folder, model, dim, name):\n",
    "    noise = np.random.normal(0, 1, (dim * dim, 100))\n",
    "    gen_imgs = model.predict(noise)\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5 \n",
    "    ims(save_folder + str(name) + '.png', merge(gen_imgs,[dim, dim]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W7xJbSSnWEBy"
   },
   "source": [
    "Output folder for new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P0lmWj3lNXK4"
   },
   "outputs": [],
   "source": [
    "# save_folder = 'drive/My Drive/Colab Notebooks/GAN/generated/'\n",
    "save_folder = 'generated/'\n",
    "model_name = 'gen_28.h5'\n",
    "model_type = 'generator'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = load_model('GAN_models/EXTENDED_200_EPOCH_64_ALLDATA/', model_name, 'generator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVh8ksaKWHcq"
   },
   "source": [
    "Generate 10x10 images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1308,
     "status": "ok",
     "timestamp": 1552761956639,
     "user": {
      "displayName": "Андрей",
      "photoUrl": "",
      "userId": "10919957261353634416"
     },
     "user_tz": -120
    },
    "id": "tMZ0EN_tRw8P",
    "outputId": "30483edb-f648-4f43-edb6-eebad2ce81ba"
   },
   "outputs": [],
   "source": [
    "generate_image(save_folder, generator, 10, 'test')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Keras_GAN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
